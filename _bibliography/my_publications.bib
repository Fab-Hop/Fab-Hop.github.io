@inproceedings{hoppe_deep_2021,
	title = {Deep {Learning} {Meets} {Knowledge} {Graphs} for {Scholarly} {Data} {Classification}},
	booktitle = {Companion {Proceedings} of the {Web} {Conference} 2021},
	author = {Hoppe, Fabian and Dessì, Danilo and Sack, Harald},
	year = {2021},
    abstract = {The amount of scientific literature continuously grows, which poses an increasing challenge for researchers to manage, find and explore research results. Therefore, the classification of scientific work is widely applied to enable the retrieval, support the search of suitable reviewers during the reviewing process, and in general to organize the existing literature according to a given schema. The automation of this classification process not only simplifies the submission process for authors, but also ensures the coherent assignment of classes. However, especially fine-grained classes and new research fields do not provide sufficient training data to automatize the process. Additionally, given the large number of not mutual exclusive classes, it is often difficult and computationally expensive to train models able to deal with multi-class multi-label settings. To overcome these issues, this work presents a preliminary Deep Learning framework as a solution for multi-label text classification for scholarly papers about Computer Science. The proposed model addresses the issue of insufficient data by utilizing the semantics of classes, which is explicitly provided by latent representations of class labels. This study uses Knowledge Graphs as a source of these required external class definitions by identifying corresponding entities in DBpedia to improve the overall classification.},
}
@inproceedings{alam_hierclassart_2021,
	title = {{HierClasSArt}: {Knowledge}-{Aware} {Hierarchical} {Classification} of {Scholarly} {Articles}},
	booktitle = {Companion {Proceedings} of the {Web} {Conference} 2021},
	author = {Alam, Mehwish and Biswas, Russa and Chen, Yiyi and Dessì, Danilo and Gesese, Genet Asefa and Hoppe, Fabian and Sack, Harald},
	year = {2021},
    doi = {10.1145/3442442.3451365},
    pages = {436–440},
    keywords = {Deep Learning, Scholarly Data, Knowledge Graphs, Hierarchical Classification},
    abstract = {A huge number of scholarly articles published every day in different domains makes it hard for the experts to organize and stay updated with the new research in a particular domain. This study gives an overview of a new approach, HierClasSArt, for knowledge aware hierarchical classification of the scholarly articles for mathematics into a predefined taxonomy. The method uses combination of neural networks and Knowledge Graphs for better document representation along with the meta-data information. This position paper further discusses the open problems about incorporation of new articles and evolving hierarchies in the pipeline. Mathematics domain has been used as a use-case.},
}

@inproceedings{hoppe_challenges_2020,
	title = {The {Challenges} of {German} {Archival} {Document} {Categorization} on {Insufficient} {Labeled} {Data}},
	booktitle = {Proceedings of the {Third} {Workshop} on {Humanities} in the {Semantic} {Web} ({WHiSe} 2020), co-located with 15th {Extended} {Semantic} {Web} {Conference} ({ESWC} 2020)},
	author = {Hoppe, Fabian and Tietz, Tabea and Dessì, Danilo and Meyer, Nils and Sprau, Mirjam and Alam, Mehwish and Sack, Harald},
	year = {2020},
    abstract = {Document exploration in archives is often challenging due to the lack of organization in topic-based categories. Moreover, archival records only provide short text which is often insufficient for capturing the semantic. This paper proposes and explores a dataless categorization approach that utilizes word embeddings and TF-IDF to categorize archival documents. Additionally, it introduces a visual approach built on top of the word embeddings to enhance the exploration of data. Preliminary results suggest that current vector representations alone do not provide enough external knowledge to solve this task. },
}

@inproceedings{gesese_leveraging_2020,
	title = {Leveraging {Multilingual} {Descriptions} for {Link} {Prediction}: {Initial} {Experiments}},
	booktitle = {Proceedings of the {ISWC} 2020 {Demos} and {Industry} {Tracks}: {From} {Novel} {Ideas} to {Industrial} {Practice}, co-located with 19th {International} {Semantic} {Web} {Conference} ({ISWC} 2020)},
	author = {Gesese, Genet Asefa and Hoppe, Fabian and Alam, Mehwish and Sack, Harald},
	year = {2020},
    abstract = {In most Knowledge Graphs (KGs), textual descriptions of entities are provided in multiple natural languages. Additional information that is not explicitly represented in the structured part of the KG might be available in these textual descriptions. Link prediction models which make use of entity descriptions usually consider only one language. However, descriptions given in multiple languages may provide complementary information which should be taken into consideration for the tasks such as link prediction. In this poster paper, the benefits of multilingual embeddings for incorporating multilingual entity descriptions into the task of link prediction in KGs are investigated.},
}
@inproceedings{hoppe_understanding_2021,
	title = {Understanding {Class} {Representations}: {An} {Intrinsic} {Evaluation} of {Zero}-{Shot} {Text} {Classification}},
	booktitle = {Workshop on {Deep} {Learning} for {Knowledge} {Graphs} ({DL4KG} @ {ISWC2021})},
	author = {Hoppe, Fabian and Dessì, Danilo and Sack, Harald},
	year = {2021},
    abstract = {Frequently, Text Classification is limited by insufficient training data. This problem is addressed by Zero-Shot Classification through the inclusion of external class definitions and then exploiting the relations between classes seen during training and unseen classes (Zero-shot). However, it requires a class embedding space capable of accurately representing the semantic relatedness between classes. This work defines an intrinsic evaluation based on greater-than constraints to provide a better understanding of this relatedness. The results imply that textual embeddings are able to capture more semantics than Knowledge Graph embeddings, but combining both modalities yields the best performance.},
}
@inproceedings{hoppe_improving_2022,
	title = {Improving {Zero}-{Shot} {Text} {Classification} with {Graph}-based {Knowledge} {Representations}},
	booktitle = {{Proceedings} of the {Doctoral} {Consortium} at {ISWC} 2022, co-located with 21st {International} {Semantic} {Web} {Conference} ({ISWC} 2022).},
	author = {Hoppe, Fabian},
	year = {2022},
    abstract = {Insufficient training data is a key challenge for text classification. In particular, long-tail class distributions and emerging, new classes do not provide any training data for specific classes. Therefore, such a zeroshot setting must incorporate additional, external knowledge to enable transfer learning by connecting the external knowledge of previously unseen classes to texts. Recent zero-shot text classifier utilize only distributional semantics defined by large language models and based on class names or natural language descriptions. This implicit knowledge contains ambiguities, is not able to capture logical relations nor is it an efficient representation of factual knowledge. These drawbacks can be avoided by introducing explicit, external knowledge. Especially, knowledge graphs provide such explicit, unambiguous, and complementary, domain specific knowledge. Hence, this thesis explores graph-based knowledge as additional modality for zero-shot text classification. Besides a general investigation of this modality, the influence on the capabilities of dealing with domain shifts by including domain-specific knowledge is explored.},
}
@inproceedings{brate_improving_2022,
	title = {Improving {Language} {Model} {Predictions} via {Prompts} {Enriched} with {Knowledge} {Graphs}},
	booktitle = {Workshop on {Deep} {Learning} for {Knowledge} {Graphs} ({DL4KG} @ {ISWC2022})},
	author = {Brate, Ryan and Dang, Minh-Hoang and Hoppe, Fabian and He, Yuan and Meroño-Peñuela, Albert and Sadashivaiah, Vijay},
	year = {2022},
    abstract = {Despite advances in deep learning and knowledge graphs (KGs), using language models for natural language understanding and question answering remains a challenging task. Pre-trained language models (PLMs) have shown to be able to leverage contextual information, to complete cloze prompts, next sentence completion and question answering tasks in various domains. Unlike structured data querying in e.g. KGs, mapping an input question to data that may or may not be stored by the language model is not a simple task. Recent studies have highlighted the improvements that can be made to the quality of information retrieved from PLMs by adding auxiliary data to otherwise naive prompts. In this paper, we explore the effects of enriching prompts with additional contextual information leveraged from the Wikidata KG on language model performance. Specifically, we compare the performance of naive vs. KG-engineered cloze prompts for entity genre classification in the movie domain. Selecting a broad range of commonly available Wikidata properties, we show that enrichment of cloze-style prompts with Wikidata information can result in a significantly higher recall for the investigated BERT and RoBERTa large PLMs. However, it is also apparent that the optimum level of data enrichment differs between models. },
}
